{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56435cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Model Accuracy: 0.9994997498749375\n",
      "Prediction counts by Claim Status: Adjusted      27\n",
      "Denied       167\n",
      "Paid          52\n",
      "Pending     1753\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Loading the data\n",
    "df = pd.read_csv(\"/Users/pavithragunasekaran/Documents/sem_3/Dab311_Deep_Learning/venv/capstone_project/Final data.csv\")\n",
    "\n",
    "\n",
    "# # Map labels: Approved = 1, Denied = 0\n",
    "# df[\"Claim Status\"] = df[\"Claim Status\"].map({\"Denied\": 0, \"Paid\": 1, \"Pending\":2,\"Adjusted\":3}) #maps the categories to numbers \n",
    "\n",
    "\n",
    "# 2. Separate features X and target Y\n",
    "X = df.drop(columns=[\"Claim Status\"])\n",
    "y = df[\"Claim Status\"]\n",
    "\n",
    "# 3. Identify numeric and categorical columns using dtypes\n",
    "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "# 4. Preprocessing\n",
    "preprocess = ColumnTransformer([ #column transformer is to convert the numerical columns to categorical \n",
    "    (\"num\", SimpleImputer(strategy=\"median\"), num_cols), #it handles missing values and filled with median\n",
    "    (\"cat\", Pipeline([ #pipeline is used to combile multiple data-processing and modeling steps into one workflow\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")) #OneHot encoder is used to convert categorical variables into binary numeric columns (0/1)\n",
    "    ]), cat_cols)\n",
    "])\n",
    "\n",
    "# 5. Logistic Regression model\n",
    "model = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"logistic\", LogisticRegression(max_iter=5000))\n",
    "])\n",
    "\n",
    "# 6. splitting the data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y # stratify ensures the categories like Approved vs Denied are evenly represented in both splits.\n",
    ")\n",
    "\n",
    "# 7. Training the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 8. Predicting and evaluating the model\n",
    "pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "\n",
    "print(\"Logistic Model Accuracy:\",accuracy)\n",
    "\n",
    "# Counting predictions by class\n",
    "prediction_counts = pd.Series(pred).value_counts().sort_index() #converts model predictions into a pandas Series.\n",
    "\n",
    "# print(\"Prediction counts (0 = Denied, 1 = Paid, 2=Pending,3=Adjusted):\")\n",
    "print(\"Prediction counts by Claim Status:\",prediction_counts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677ceaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with Denied/Pending: 8770\n",
      "Rows with CO-119: 5511\n",
      "Files saved in folder: outputs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data = \"/Users/pavithragunasekaran/Documents/sem_3/Dab311_Deep_Learning/venv/capstone_project/Final data.csv\"\n",
    "status_col = \"Claim Status\"\n",
    "reasoncode = \"Reason Code\"\n",
    "output_directory = \"outputs\"\n",
    "\n",
    "def main():\n",
    "    os.makedirs(output_directory, exist_ok=True) #creating a directory called output_directory to store all the seperated files.\n",
    "\n",
    "    df = pd.read_csv(data)\n",
    "\n",
    "    # Standardizing the claim status into lowercase and remove the spaces to make system to learn easily\n",
    "    df[\"_status_norm\"] = (\n",
    "        df[status_col]\n",
    "        .astype(str)\n",
    "        .str.strip() #removes the extra spaces which is in the claim_status\n",
    "        .str.lower()\n",
    "    )\n",
    "\n",
    "    #  Denied OR pending claims\n",
    "    denied_pending_df = df[\n",
    "        df[\"_status_norm\"].str.contains(\"deny\", na=False) | \n",
    "        df[\"_status_norm\"].str.contains(\"pend\", na=False)\n",
    "    ]\n",
    "\n",
    "    # save denied and pending data for all reason code in one folder called output directory and naming the file as \"denied_and_pending_claims.csv\"\n",
    "    denied_pending_data = os.path.join(output_directory, \"denied_and_pending_claims.csv\")\n",
    "    denied_pending_df.to_csv(denied_pending_data, index=False)\n",
    "\n",
    "    # seperating the co-119 data only from the denied_and_pending_claims.csv file\n",
    "    co119_df = denied_pending_df[\n",
    "        denied_pending_df[reasoncode]\n",
    "        .astype(str)\n",
    "        .str.upper() #converts the reason codes to uppercase\n",
    "        .str.replace(\" \", \"\")#removes any spaces\n",
    "        .isin([\"CO-119\", \"CO119\"]) #keeps only rows where the reason code exactly matches CO-119, CO119\n",
    "    ]\n",
    "\n",
    " #save denied and pending data for CO119 reason code in one folder called output directory and naming the file as \"co119_denied_pending_claims.csv\"\n",
    "    co119_data = os.path.join(output_directory, \"co119_denied_pending_claims.csv\")\n",
    "    co119_df.to_csv(co119_data, index=False)\n",
    "\n",
    "    print(\"Rows with Denied/Pending:\", len(denied_pending_df))\n",
    "    print(\"Rows with CO-119:\", len(co119_df))\n",
    "    print(\"Files saved in folder:\", output_directory)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() #checks whether the Python file is run directly. if the file is imported into another notebook, main() will not run automatically, which prevents unintended execution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
